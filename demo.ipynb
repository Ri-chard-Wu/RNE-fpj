{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define unet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear: # no.\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    " \n",
    "# else, obstacle, path.\n",
    "colorMap = [[254, 235, 206], [58, 64, 175], [50, 39, 24]] # BGR.\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear \n",
    "        \n",
    "        self.inc = (DoubleConv(n_channels, 32))\n",
    "        self.down1 = (Down(32, 64))\n",
    "        self.down2 = (Down(64, 128))\n",
    "        self.down3 = (Down(128, 256))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = (Down(256, 512 // factor))\n",
    "        self.up1 = (Up(512, 256 // factor, bilinear))\n",
    "        self.up2 = (Up(256, 128 // factor, bilinear))\n",
    "        self.up3 = (Up(128, 64 // factor, bilinear))\n",
    "        self.up4 = (Up(64, 32, bilinear))\n",
    "        self.outc = (OutConv(32, n_classes))\n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def use_checkpointing(self): \n",
    "        self.inc = torch.utils.checkpoint(self.inc)\n",
    "        self.down1 = torch.utils.checkpoint(self.down1)\n",
    "        self.down2 = torch.utils.checkpoint(self.down2)\n",
    "        self.down3 = torch.utils.checkpoint(self.down3)\n",
    "        self.down4 = torch.utils.checkpoint(self.down4)\n",
    "        self.up1 = torch.utils.checkpoint(self.up1)\n",
    "        self.up2 = torch.utils.checkpoint(self.up2)\n",
    "        self.up3 = torch.utils.checkpoint(self.up3)\n",
    "        self.up4 = torch.utils.checkpoint(self.up4)\n",
    "        self.outc = torch.utils.checkpoint(self.outc)\n",
    "\n",
    " \n",
    " \n",
    "    def predict(self, image):   \n",
    "        '''\n",
    "            image: (3, 64, 64), BGR, numpy array. \n",
    "        '''             \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # (3, 64, 64), RGB.\n",
    "        image = torch.from_numpy(image).to(device=device, dtype=torch.float32,\n",
    "                                 memory_format=torch.channels_last).unsqueeze(0) # (1, 3, 64, 64), RGB.\n",
    "        \n",
    "        mask_pred = self(image).argmax(dim=1)[0] # (64, 64).\n",
    "        \n",
    "        # image = np.transpose(image.detach().cpu().numpy()[0], (1,2,0))            \n",
    "        mask_pred = mask_pred.detach().cpu().numpy() # (64, 64).\n",
    "            \n",
    "        mask3 = np.zeros((64, 64, 3)).astype(np.uint8)\n",
    "        for clsId in range(self.n_classes):\n",
    "            mask3[mask_pred==clsId] = np.array(colorMap[clsId])\n",
    "\n",
    "        mask3 = np.transpose(mask3, (2, 0, 1)) # (3, 64, 64), BGR.\n",
    "                    \n",
    "        return mask3 \n",
    "    \n",
    "    \n",
    "\n",
    "    def load(self, dir_name, name):\n",
    "\n",
    "        path = os.path.join(dir_name, name)\n",
    "\n",
    "        state_dict = torch.load(path)\n",
    "      \n",
    "        self.load_state_dict(state_dict) \n",
    "    \n",
    "        print(f'loaded unet ckpt: {path}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load unet ckeckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unet = UNet(n_channels=3, n_classes=3, bilinear=False)\n",
    "unet.load('ckpt', 'unet.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test unet's performance. It will turn on the camera, feed camera's captured image to unet, and then save unet's predicted segmentation mask to `unet_output` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Create data folder.\n",
    "image_folder = \"unet_output/\"\n",
    "if not os.path.exists(image_folder):\n",
    "    os.makedirs(image_folder)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(camera_value): \n",
    "\n",
    "    global unet\n",
    "    \n",
    "    x = camera_value # (h, w, c), BGR. \n",
    "\n",
    "    img = Image.fromarray(x)\n",
    "    img = img.resize((64, 64), Image.BILINEAR)\n",
    "    img = np.array(img) # (64, 64, 3), BGR.  \n",
    "  \n",
    "    mask = unet.predict(np.transpose(img, (2, 0, 1))) # (3, 64, 64), BGR.\n",
    " \n",
    "    mask = cv2.cvtColor(np.transpose(mask, (1, 2, 0)), cv2.COLOR_BGR2RGB) # (64, 64, 3), RGB.\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # (64, 64, 3), RGB.  \n",
    "\n",
    "    return img, mask\n",
    "\n",
    "\n",
    "\n",
    "# Save image when camera update the vlaue.\n",
    "def update(change):\n",
    "\n",
    "    global frame_id, image_folder\n",
    "    fname = str(frame_id).zfill(4) + \".jpg\"\n",
    "\n",
    "\n",
    "    img, mask = preprocess(change['new']) # (3, 64, 64), BGR.\n",
    "    \n",
    "\n",
    "    plt.subplot(121) \n",
    "    plt.imshow(img) \n",
    "    plt.axis('off') \n",
    "    plt.title(\"input image\") \n",
    "\n",
    "    plt.subplot(122) \n",
    "    plt.imshow(mask)\n",
    "    plt.axis('off') \n",
    "    plt.title(\"mask\") \n",
    "\n",
    "    plt.savefig(image_folder + fname) \n",
    "\n",
    "    print(\"\\rsave data \" + fname, end=\"\")\n",
    "    frame_id += 1\n",
    "    \n",
    "    time.sleep(0.2)\n",
    "\n",
    " \n",
    "\n",
    "# Activate the camera.\n",
    "camera = Camera.instance(width=960, height=540, capture_width=1280, capture_height=720)\n",
    "image = widgets.Image(format='jpeg', width=480, height=270)  # this width and height doesn't necessarily have to match the camera\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    " \n",
    "\n",
    "# Start collect & segment data.\n",
    "frame_id = 0\n",
    "camera.observe(update, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve(update, names='value')\n",
    "time.sleep(0.1)\n",
    "camera_link.unlink()\n",
    "camera.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import os \n",
    "  \n",
    "import cv2\n",
    "   \n",
    "import torch.nn as nn \n",
    "from torch.distributions import Normal\n",
    "\n",
    "  \n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "action_low = np.array([0.0, 0.0])\n",
    "action_high = np.array([0.5, 0.5])\n",
    "\n",
    "action_dim = 2\n",
    "obs_shape = (64, 64, 3)\n",
    " \n",
    "DEVICE = \"cpu\"\n",
    " \n",
    "LOG_SIG_MAX = 2\n",
    "LOG_SIG_MIN = -20\n",
    "epsilon = 1e-6\n",
    "\n",
    "\n",
    "sac_args = AttrDict({      \n",
    "    'hidden_size': 512, \n",
    "    'input_dim': 32    \n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "def action_rescale(action):\n",
    "    action = action_low + (action + 1.0) * 0.5 * (action_high - action_low)\n",
    "    return np.clip(action, action_low, action_high)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Policy weights\n",
    "def weights_init_(m):\n",
    " \n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "   \n",
    "    def __init__(self, in_channels: int, \n",
    "                        out_channels: int,\n",
    "                        kernel_size: int, \n",
    "                        stride: int = 2, \n",
    "                        padding: int = 1, \n",
    "                        slope: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=slope)\n",
    "    \n",
    "    def forward(self, x):\n",
    "  \n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    " \n",
    "\n",
    "class Encoder(nn.Module):\n",
    " \n",
    "    def __init__(self, z_dim: int = 32):\n",
    "        super().__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(OrderedDict([\n",
    "            (\"conv1\", nn.Conv2d(3, 32, 4, stride=2, padding=1)),\n",
    "            (\"relu1\", nn.LeakyReLU(0.2)), #32x32x32\n",
    "            (\"block1\", ConvBlock(32, 64, 4, stride=2, padding=1, slope=0.2)), # 64x16x16\n",
    "            (\"block2\", ConvBlock(64, 128, 4, stride=2, padding=1, slope=0.2)), # 128x8x8\n",
    "            (\"block3\", ConvBlock(128, 256, 4, stride=2, padding=1, slope=0.2)), # 256x4x4\n",
    "        ]))\n",
    "\n",
    "        self.fc = nn.Linear(4096, z_dim)\n",
    "      \n",
    " \n",
    "    def forward(self, x):\n",
    " \n",
    "        x = self.encoder(x)\n",
    "        x = x.view(-1, 4096)\n",
    "        return self.fc(x) \n",
    "\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc = Encoder(num_inputs)\n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_dim) # 32, 512.\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim) # 512, 512.\n",
    "\n",
    "        self.mean_linear = nn.Linear(hidden_dim, num_actions) # 512, 2.\n",
    "        self.log_std_linear = nn.Linear(hidden_dim, num_actions) # 512, 2.\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "\n",
    "    def forward(self, state, vel):\n",
    "        \n",
    "        # state = self.enc(state)\n",
    "        x = torch.cat([self.enc(state), vel], 1)        \n",
    "\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        mean = self.mean_linear(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, min=LOG_SIG_MIN, max=LOG_SIG_MAX)\n",
    "        return mean, log_std\n",
    "\n",
    "\n",
    "    def sample(self, state, vel):\n",
    "        mean, log_std = self.forward(state, vel)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal(mean, std)\n",
    "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
    "        action = torch.tanh(x_t)\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "\n",
    "        # Enforcing Action Bound\n",
    "        log_prob -= torch.log(1 - action.pow(2) + epsilon)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "\n",
    "        return action, log_prob, torch.tanh(mean) # (b, act_dim), (b, 1), (b, act_dim). \n",
    "\n",
    " \n",
    " \n",
    "class SAC(object):\n",
    " \n",
    "    def __init__(self):\n",
    "           \n",
    "        self.policy = GaussianPolicy(sac_args.input_dim, action_dim, sac_args.hidden_size).to(DEVICE)\n",
    "         \n",
    "      \n",
    "    def select_action(self, obs, vel):\n",
    "        '''\n",
    "            obs: BGR, (3, 64, 64).\n",
    "        '''\n",
    "\n",
    "        obs = torch.FloatTensor(obs).to(DEVICE).unsqueeze(0) # (1, 3, 64, 64).\n",
    "        vel = torch.FloatTensor(vel).to(DEVICE).unsqueeze(0) # (1, 2).\n",
    "\n",
    "        assert tuple(obs.shape) == (1, 3, 64, 64)\n",
    "\n",
    "        action, _, _ = self.policy.sample(obs, vel)\n",
    "\n",
    "        action = action.detach().cpu().numpy() # (1, 2).\n",
    "        assert  action.shape == (1, action_dim)\n",
    "\n",
    "\n",
    "        return action_rescale(action[0]) # (2,).\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "    def load(self, dir_name, name):\n",
    "\n",
    "        path = os.path.join(dir_name, name)\n",
    "\n",
    "        checkpoint = torch.load(path)\n",
    "      \n",
    "        self.policy.load_state_dict(checkpoint['policy']) \n",
    "    \n",
    "        print(f'loaded ckpt: {path}')\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SAC()\n",
    "\n",
    "agent.load('ckpt', 'sac.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traitlets\n",
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "camera = Camera.instance(width=224, height=224)\n",
    "image = widgets.Image(format='jpeg', width=224, height=224)\n",
    "blocked_slider = widgets.FloatSlider(description='blocked', min=0.0, max=1.0, orientation='vertical')\n",
    "speed_slider = widgets.FloatSlider(description='speed', min=0.0, max=0.5, value=0.0, step=0.01, orientation='horizontal')\n",
    "\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "display(widgets.VBox([widgets.HBox([image, blocked_slider]), speed_slider]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from jetbot import Robot\n",
    "\n",
    "\n",
    "robot = Robot() \n",
    "robot.left(speed=0)\n",
    "robot.right(speed=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    " \n",
    "\n",
    " \n",
    "cur_vel = np.array([0.0, 0.0])\n",
    "\n",
    "\n",
    " \n",
    "def preprocess(camera_value): \n",
    "    '''\n",
    "        Semantic segmentation using U-Net.\n",
    "    '''\n",
    "    global unet\n",
    "    \n",
    "    x = camera_value # (h, w, c), BGR. \n",
    "\n",
    "    # TODO: distorsion calibration.\n",
    "\n",
    "    img = Image.fromarray(x)\n",
    "    img = img.resize((64, 64), Image.BILINEAR)\n",
    "    img = np.array(img) # (64, 64, 3), BGR.  \n",
    " \n",
    "    img = np.transpose(img, (2, 0, 1)) # (3, 64, 64), BGR. \n",
    "\n",
    "    img = unet.predict(img) # (3, 64, 64), BGR.\n",
    "\n",
    "    return img\n",
    "\n",
    "  \n",
    "  \n",
    "def update(change):\n",
    "\n",
    "    global robot, agent\n",
    "    \n",
    "    cur_obs = preprocess(change['new']) # (3, 64, 64), BGR.\n",
    "    \n",
    "    # cur_obs: BGR, (3, 64, 64).\n",
    "    cur_vel = agent.select_action(cur_obs, cur_vel)\n",
    "\n",
    "    robot.left(speed=cur_vel[0])\n",
    "    robot.right(speed=cur_vel[1])\n",
    "      \n",
    "    time.sleep(0.001) # If performnace is not good, try tuning this delay.\n",
    "        \n",
    "\n",
    "update({'new': camera.value})  # we call the function once to initialize\n",
    "\n",
    "camera.observe(update, names='value')  # this attaches the 'update' function to the 'value' traitlet of our camera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve(update, names='value')\n",
    "time.sleep(0.1)  # add a small sleep to make sure frames have finished processing\n",
    "\n",
    "robot.left(speed=0)\n",
    "robot.right(speed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "robot.stop()\n",
    "\n",
    "camera_link.unlink()  # don't stream to browser (will still run camera)\n",
    "# camera_link.link()  # stream to browser (wont run camera)\n",
    "\n",
    "camera.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
