{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import os \n",
    "  \n",
    "import cv2\n",
    "   \n",
    "import torch.nn as nn \n",
    "from torch.distributions import Normal\n",
    "\n",
    "  \n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "action_low = np.array([0.0, 0.0])\n",
    "action_high = np.array([0.5, 0.5])\n",
    "\n",
    "action_dim = 2\n",
    "obs_shape = (64, 64, 3)\n",
    " \n",
    "DEVICE = \"cpu\"\n",
    " \n",
    "LOG_SIG_MAX = 2\n",
    "LOG_SIG_MIN = -20\n",
    "epsilon = 1e-6\n",
    "\n",
    "\n",
    "sac_args = AttrDict({      \n",
    "    'hidden_size': 512, \n",
    "    'input_dim': 32    \n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "def action_rescale(action):\n",
    "    action = action_low + (action + 1.0) * 0.5 * (action_high - action_low)\n",
    "    return np.clip(action, action_low, action_high)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Policy weights\n",
    "def weights_init_(m):\n",
    " \n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "   \n",
    "    def __init__(self, in_channels: int, \n",
    "                        out_channels: int,\n",
    "                        kernel_size: int, \n",
    "                        stride: int = 2, \n",
    "                        padding: int = 1, \n",
    "                        slope: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=slope)\n",
    "    \n",
    "    def forward(self, x):\n",
    "  \n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    " \n",
    "\n",
    "class Encoder(nn.Module):\n",
    " \n",
    "    def __init__(self, z_dim: int = 32):\n",
    "        super().__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(OrderedDict([\n",
    "            (\"conv1\", nn.Conv2d(3, 32, 4, stride=2, padding=1)),\n",
    "            (\"relu1\", nn.LeakyReLU(0.2)), #32x32x32\n",
    "            (\"block1\", ConvBlock(32, 64, 4, stride=2, padding=1, slope=0.2)), # 64x16x16\n",
    "            (\"block2\", ConvBlock(64, 128, 4, stride=2, padding=1, slope=0.2)), # 128x8x8\n",
    "            (\"block3\", ConvBlock(128, 256, 4, stride=2, padding=1, slope=0.2)), # 256x4x4\n",
    "        ]))\n",
    "\n",
    "        self.fc = nn.Linear(4096, z_dim)\n",
    "      \n",
    " \n",
    "    def forward(self, x):\n",
    " \n",
    "        x = self.encoder(x)\n",
    "        x = x.view(-1, 4096)\n",
    "        return self.fc(x) \n",
    "\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc = Encoder(num_inputs)\n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_dim) # 32, 512.\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim) # 512, 512.\n",
    "\n",
    "        self.mean_linear = nn.Linear(hidden_dim, num_actions) # 512, 2.\n",
    "        self.log_std_linear = nn.Linear(hidden_dim, num_actions) # 512, 2.\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "\n",
    "    def forward(self, state, vel):\n",
    "        \n",
    "        # state = self.enc(state)\n",
    "        x = torch.cat([self.enc(state), vel], 1)        \n",
    "\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        mean = self.mean_linear(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, min=LOG_SIG_MIN, max=LOG_SIG_MAX)\n",
    "        return mean, log_std\n",
    "\n",
    "\n",
    "    def sample(self, state, vel):\n",
    "        mean, log_std = self.forward(state, vel)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal(mean, std)\n",
    "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
    "        action = torch.tanh(x_t)\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "\n",
    "        # Enforcing Action Bound\n",
    "        log_prob -= torch.log(1 - action.pow(2) + epsilon)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "\n",
    "        return action, log_prob, torch.tanh(mean) # (b, act_dim), (b, 1), (b, act_dim). \n",
    "\n",
    " \n",
    " \n",
    "class SAC(object):\n",
    " \n",
    "    def __init__(self):\n",
    "           \n",
    "        self.policy = GaussianPolicy(sac_args.input_dim, action_dim, sac_args.hidden_size).to(DEVICE)\n",
    "         \n",
    "      \n",
    "    def select_action(self, obs, vel):\n",
    "        '''\n",
    "            obs: BGR, (3, 64, 64).\n",
    "        '''\n",
    "\n",
    "        obs = torch.FloatTensor(obs).to(DEVICE).unsqueeze(0) # (1, 3, 64, 64).\n",
    "        vel = torch.FloatTensor(vel).to(DEVICE).unsqueeze(0) # (1, 2).\n",
    "\n",
    "        assert tuple(obs.shape) == (1, 3, 64, 64)\n",
    "\n",
    "        action, _, _ = self.policy.sample(obs, vel)\n",
    "\n",
    "        action = action.detach().cpu().numpy() # (1, 2).\n",
    "        assert  action.shape == (1, action_dim)\n",
    "\n",
    "\n",
    "        return action_rescale(action[0]) # (2,).\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "    def load(self, dir_name, name):\n",
    "\n",
    "        path = os.path.join(dir_name, name)\n",
    "\n",
    "        checkpoint = torch.load(path)\n",
    "      \n",
    "        self.policy.load_state_dict(checkpoint['policy']) \n",
    "    \n",
    "        print(f'loaded ckpt: {path}')\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SAC()\n",
    "\n",
    "agent.load('Python-Wrapper/ckpt', 'model_600-r2800')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UNet(object):\n",
    " \n",
    "    def __init__(self):\n",
    "           \n",
    "        pass\n",
    "\n",
    "\n",
    "    def load(self, dir_name, name):\n",
    "\n",
    "        pass\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet()\n",
    "\n",
    "unet.load('', '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traitlets\n",
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "camera = Camera.instance(width=224, height=224)\n",
    "image = widgets.Image(format='jpeg', width=224, height=224)\n",
    "blocked_slider = widgets.FloatSlider(description='blocked', min=0.0, max=1.0, orientation='vertical')\n",
    "speed_slider = widgets.FloatSlider(description='speed', min=0.0, max=0.5, value=0.0, step=0.01, orientation='horizontal')\n",
    "\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "display(widgets.VBox([widgets.HBox([image, blocked_slider]), speed_slider]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from jetbot import Robot\n",
    "\n",
    "\n",
    "robot = Robot() \n",
    "robot.left(speed=0)\n",
    "robot.right(speed=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    " \n",
    "\n",
    " \n",
    "cur_vel = np.array([0.0, 0.0])\n",
    "\n",
    "\n",
    "def preprocess(camera_value): \n",
    "    '''\n",
    "        Semantic segmentation using U-Net.\n",
    "    '''\n",
    "    global unet\n",
    "    \n",
    "    x = camera_value # (h, w, c), BGR. \n",
    "\n",
    "    img = Image.fromarray(x)\n",
    "    img = img.resize((64, 64), Image.BILINEAR)\n",
    "    img = np.array(img) # (64, 64, 3) \n",
    " \n",
    "    img = np.transpose(img, (2, 0, 1)) # (c, h, w).\n",
    "\n",
    "    img = unet(img) # TODO: Implement unet semantic segmentation.\n",
    "\n",
    "    return img\n",
    "\n",
    "  \n",
    "  \n",
    "def update(change):\n",
    "\n",
    "    global robot, agent\n",
    "    \n",
    "    cur_obs = preprocess(change['new'])\n",
    "    \n",
    "    cur_vel = agent.select_action(cur_obs, cur_vel)\n",
    "\n",
    "    robot.left(speed=cur_vel[0])\n",
    "    robot.right(speed=cur_vel[1])\n",
    "      \n",
    "    time.sleep(0.001) # If performnace is not good, try tuning this delay.\n",
    "        \n",
    "\n",
    "update({'new': camera.value})  # we call the function once to initialize\n",
    "\n",
    "camera.observe(update, names='value')  # this attaches the 'update' function to the 'value' traitlet of our camera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve(update, names='value')\n",
    "time.sleep(0.1)  # add a small sleep to make sure frames have finished processing\n",
    "\n",
    "robot.left(speed=0)\n",
    "robot.right(speed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "robot.stop()\n",
    "\n",
    "camera_link.unlink()  # don't stream to browser (will still run camera)\n",
    "# camera_link.link()  # stream to browser (wont run camera)\n",
    "\n",
    "camera.stop()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
